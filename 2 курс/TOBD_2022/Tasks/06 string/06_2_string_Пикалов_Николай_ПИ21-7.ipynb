{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMVA8ysehRBL"
   },
   "source": [
    "5\\. В текстах шагов рецептов обыкновенные дроби имеют вид \"a / b\". Используя регулярные выражения, уберите в тексте шагов рецепта с id 72367 пробелы до и после символа дроби. Выведите на экран шаги этого рецепта после их изменения."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df_recipes = pd.read_csv('recipes_sample.csv')\n",
    "\n",
    "with open('steps_sample.xml') as f:\n",
    "  data = BeautifulSoup(f, 'xml')\n",
    "\n",
    "steps_dict = {}\n",
    "\n",
    "for recipe in data.find_all('recipe'):\n",
    "  recipe_id = int(recipe.id.text)\n",
    "  steps_dict[recipe_id] = [step.text for step in recipe.find_all('step')]\n",
    "\n",
    "for step in steps_dict[72367]:\n",
    "  print(re.sub(r'\\d+ / \\d+', '\\1/\\2', step, count=0))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jvqvx2tijV0w",
    "outputId": "46e305f9-ecd6-4455-d5dd-9931e79e49d6"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mix butter , flour , \u0001/\u0002 c\n",
      "sugar and 1-\u0001/\u0002 t\n",
      "vanilla\n",
      "press into greased 9\" springform pan\n",
      "mix cream cheese , \u0001/\u0002 c\n",
      "sugar , eggs and \u0001/\u0002 t\n",
      "vanilla beating until fluffy\n",
      "pour over dough\n",
      "combine apples , \u0001/\u0002 c\n",
      "sugar and cinnamon\n",
      "arrange on top of cream cheese mixture and sprinkle with almonds\n",
      "bake at 350 for 45-55 minutes , or until tester comes out clean\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPpqav9khRBM"
   },
   "source": [
    "### Сегментация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ny6CfFzhRBN"
   },
   "source": [
    "6\\. Разбейте тексты шагов рецептов на слова при помощи пакета `nltk`. Посчитайте и выведите на экран кол-во уникальных слов среди всех рецептов. Словом называется любая последовательность алфавитных символов (для проверки можно воспользоваться `str.isalpha`). При подсчете количества уникальных слов не учитывайте регистр."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk"
   ],
   "metadata": {
    "id": "pkfh7_mbnfVS"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('all')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjH5ch_MpOIc",
    "outputId": "f07357e4-ca4e-4b80-cae4-14a409b6fe73"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "arr = steps_dict.values()\n",
    "flat_arr = [item.lower() for sublist in arr for item in sublist]"
   ],
   "metadata": {
    "id": "2DbZPs6DrMA8"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flat_arr[:3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8DMdfmasZdj",
    "outputId": "43295936-f9f9-482f-9424-56f9d5b9d22e"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['in 1 / 4 cup butter , saute carrots , onion , celery and broccoli stems for 5 minutes',\n",
       " 'add thyme , oregano and basil',\n",
       " 'saute 5 minutes more']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "res = toktok.tokenize(flat_arr)"
   ],
   "metadata": {
    "id": "KJT2bAxEqe2f"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('Количество уникальных слов:', len(set(filter(str.isalpha, res))))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTaEOvavtXKV",
    "outputId": "d118f0fe-62a7-4a60-bdb1-00b5a9829e61"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Количество уникальных слов: 14953\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gq8aM9DfhRBN"
   },
   "source": [
    "7\\. Разбейте описания рецептов из `recipes` на предложения при помощи пакета `nltk`. Найдите 5 самых длинных описаний (по количеству _предложений_) рецептов в датасете и выведите строки фрейма, соответствующие этим рецептами, в порядке убывания длины."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "df_recipes['sentence_count'] = df_recipes['description'].apply(lambda x : len(tokenizer.tokenize(x))) \n",
    "df_recipes.sort_values('sentence_count', ascending=False).head(5)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "id": "DOwtfIhMuroU",
    "outputId": "e29d9cba-0c67-4e64-ad64-8f8cefab69d2"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                                   name  \\\n",
       "18408                      my favorite buttercream icing for decorating   \n",
       "481           alligator claws  avocado fritters  with chipotle lime dip   \n",
       "22566                                         rich barley mushroom soup   \n",
       "16296  little bunny foo foo cake  carrot cake  with cream cheese frosti   \n",
       "6779                                                      chocolate tea   \n",
       "\n",
       "           id  minutes  contributor_id   submitted  n_steps  \\\n",
       "18408  334113       30          681465  2008-10-30     12.0   \n",
       "481    287008       45          765354  2008-02-19      NaN   \n",
       "22566  328708       60          221776  2008-10-03      NaN   \n",
       "16296  316000       68          689540  2008-07-27     14.0   \n",
       "6779   205348        6          428824  2007-01-14      NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           description  \\\n",
       "18408  this wonderful icing is used for icing cakes and cookies as well as for borders and art work on cakes.  it makes a delicious filling also between the layers of cakes and under fondant icing.  you can make roses but it takes 3 or more days to dry them depending on the humidity. \\r\\n\\r\\nthere are many versions of “buttercream” icing. some are made with eggs and all butter.  some varieties, you have to cook your sugar to a softball stage.  others are 100% shortening or a combination of shortening and butter.\\r\\n\\r\\neach decorator has his or her favorite.  i personally think that the best taste and textured recipe is the one that has you cook your sugar, add to whipped eggs and use pounds of butter per batch. but…. i live in a state that can easily be a 100 degrees for days on end during the summer and you know what butter does on hot days.  it melts! ...   \n",
       "481    a translucent golden-brown crust allows the green of the avocado to be seen.  the crispy exterior is a counterpoint to the unctuous interior.  these are a signature dish for me, and the one i most often get requests to make (although my seafood and ricotta stuffed buckwheat pancakes run a close second).\\r\\n\\r\\nthese fritters came about ten years ago when i was shopping for a dinner i was making for a friend who is a cia-trained chef.  i was in a vegetable market and saw these gorgeous avocados that i just knew would be ripe in the next two days.  i tried to think of what i could do with them since a) everyone serves cold avocado, and b) i really am not fond of guacamole.  as i tried to think of what i could make with them that was hot, the work 'fritters' jumped into my head.  having never made a fritter before, i was a little surprised to have tha...   \n",
       "22566  this is one of the best soups i've ever made and it is even worthy of company.  so simple, yet rich in deep, mushroomy flavor.  the inspiration was zaar #26877, a delicious mushroom rice casserole.  i found i couldn't stop eating the liquid before putting the casserole into the oven and that gave me the idea that the base  would make a delicious soup.  and it does! \\r\\nuse plenty of fresh mushrooms.  i buy them when they are marked 1/2 price at the grocery, as this is a good way to use your 'shrooms that are starting to get dark.  it is the soy sauce that transforms the broth from ho-hum to yum.  i try to use low sodium or home-made no sodium chicken broth so that i can use the soy for the sodium.  there is no sense of \"asian\" in this soup at all.  ( i would not make this without the soy. )  just a little bit adds the depth of flavor and even color...   \n",
       "16296  the first time i made this cake i grated a million pounds of carrots on a knucklebuster.  then they invented cuisinarts!  now it is much faster to shred the carrots on a fine shredding disk and no bloody knuckles!  i have baked it in 8\", 9\", 9x13\" pans so if you want to experiment with pan size it works.  one thing i found was baking and stacking the three layers is tricky.  my favorite way is two 8\" pans for a nice layer cake and an 8\" square pan to put into the freezer for unexpected company. i hope you try this wonderful cake.  update:  in the spirit of carrot cake stories, this cake was invented by a bunny named foo-foo.  he is very famous and even has a hit song which goes like this: sing to the tune of 'down by the station'..........     \\r\\n\\r\\n\\r\\n little bunny foo foo,\\r\\nhopping through the forest,\\r\\nscooping up the field mice,\\r\\nand bo...   \n",
       "6779   i wrote this because there are an astounding lack of chocolate tea recipes on the internet. \\r\\n\\r\\n the first time i heard about chocolate tea was doing a web search on chocolate. there seem to be a few companies out there who sell chocolate tea. i like to stay up late and had run out of coffee. i was in real need for a good tasting caffene beverage. i first thought chocolate tea would be yucky. we are conditioned to accept chocolate with coffee as a rule but not tea. i was very mistaken! \\r\\n\\r\\n tea and chocolate goes very well with each other and it is also very good for your body. both tea and chocolate are loaded with antioxidents. you may however not want to give this to small children because of the caffene. \\r\\n\\r\\n not having a recipe to follow, i created one. (this one) i used these ingredients because i had them on hand and it was quick...   \n",
       "\n",
       "       n_ingredients  sentence_count  \n",
       "18408            NaN              76  \n",
       "481              9.0              27  \n",
       "22566           10.0              24  \n",
       "16296            NaN              23  \n",
       "6779             NaN              23  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-df926b9c-1ebe-4264-a52c-7ad6f384fd75\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>description</th>\n",
       "      <th>n_ingredients</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18408</th>\n",
       "      <td>my favorite buttercream icing for decorating</td>\n",
       "      <td>334113</td>\n",
       "      <td>30</td>\n",
       "      <td>681465</td>\n",
       "      <td>2008-10-30</td>\n",
       "      <td>12.0</td>\n",
       "      <td>this wonderful icing is used for icing cakes and cookies as well as for borders and art work on cakes.  it makes a delicious filling also between the layers of cakes and under fondant icing.  you can make roses but it takes 3 or more days to dry them depending on the humidity. \\r\\n\\r\\nthere are many versions of “buttercream” icing. some are made with eggs and all butter.  some varieties, you have to cook your sugar to a softball stage.  others are 100% shortening or a combination of shortening and butter.\\r\\n\\r\\neach decorator has his or her favorite.  i personally think that the best taste and textured recipe is the one that has you cook your sugar, add to whipped eggs and use pounds of butter per batch. but…. i live in a state that can easily be a 100 degrees for days on end during the summer and you know what butter does on hot days.  it melts! ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>alligator claws  avocado fritters  with chipotle lime dip</td>\n",
       "      <td>287008</td>\n",
       "      <td>45</td>\n",
       "      <td>765354</td>\n",
       "      <td>2008-02-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a translucent golden-brown crust allows the green of the avocado to be seen.  the crispy exterior is a counterpoint to the unctuous interior.  these are a signature dish for me, and the one i most often get requests to make (although my seafood and ricotta stuffed buckwheat pancakes run a close second).\\r\\n\\r\\nthese fritters came about ten years ago when i was shopping for a dinner i was making for a friend who is a cia-trained chef.  i was in a vegetable market and saw these gorgeous avocados that i just knew would be ripe in the next two days.  i tried to think of what i could do with them since a) everyone serves cold avocado, and b) i really am not fond of guacamole.  as i tried to think of what i could make with them that was hot, the work 'fritters' jumped into my head.  having never made a fritter before, i was a little surprised to have tha...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22566</th>\n",
       "      <td>rich barley mushroom soup</td>\n",
       "      <td>328708</td>\n",
       "      <td>60</td>\n",
       "      <td>221776</td>\n",
       "      <td>2008-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one of the best soups i've ever made and it is even worthy of company.  so simple, yet rich in deep, mushroomy flavor.  the inspiration was zaar #26877, a delicious mushroom rice casserole.  i found i couldn't stop eating the liquid before putting the casserole into the oven and that gave me the idea that the base  would make a delicious soup.  and it does! \\r\\nuse plenty of fresh mushrooms.  i buy them when they are marked 1/2 price at the grocery, as this is a good way to use your 'shrooms that are starting to get dark.  it is the soy sauce that transforms the broth from ho-hum to yum.  i try to use low sodium or home-made no sodium chicken broth so that i can use the soy for the sodium.  there is no sense of \"asian\" in this soup at all.  ( i would not make this without the soy. )  just a little bit adds the depth of flavor and even color...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16296</th>\n",
       "      <td>little bunny foo foo cake  carrot cake  with cream cheese frosti</td>\n",
       "      <td>316000</td>\n",
       "      <td>68</td>\n",
       "      <td>689540</td>\n",
       "      <td>2008-07-27</td>\n",
       "      <td>14.0</td>\n",
       "      <td>the first time i made this cake i grated a million pounds of carrots on a knucklebuster.  then they invented cuisinarts!  now it is much faster to shred the carrots on a fine shredding disk and no bloody knuckles!  i have baked it in 8\", 9\", 9x13\" pans so if you want to experiment with pan size it works.  one thing i found was baking and stacking the three layers is tricky.  my favorite way is two 8\" pans for a nice layer cake and an 8\" square pan to put into the freezer for unexpected company. i hope you try this wonderful cake.  update:  in the spirit of carrot cake stories, this cake was invented by a bunny named foo-foo.  he is very famous and even has a hit song which goes like this: sing to the tune of 'down by the station'..........     \\r\\n\\r\\n\\r\\n little bunny foo foo,\\r\\nhopping through the forest,\\r\\nscooping up the field mice,\\r\\nand bo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>chocolate tea</td>\n",
       "      <td>205348</td>\n",
       "      <td>6</td>\n",
       "      <td>428824</td>\n",
       "      <td>2007-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i wrote this because there are an astounding lack of chocolate tea recipes on the internet. \\r\\n\\r\\n the first time i heard about chocolate tea was doing a web search on chocolate. there seem to be a few companies out there who sell chocolate tea. i like to stay up late and had run out of coffee. i was in real need for a good tasting caffene beverage. i first thought chocolate tea would be yucky. we are conditioned to accept chocolate with coffee as a rule but not tea. i was very mistaken! \\r\\n\\r\\n tea and chocolate goes very well with each other and it is also very good for your body. both tea and chocolate are loaded with antioxidents. you may however not want to give this to small children because of the caffene. \\r\\n\\r\\n not having a recipe to follow, i created one. (this one) i used these ingredients because i had them on hand and it was quick...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df926b9c-1ebe-4264-a52c-7ad6f384fd75')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-df926b9c-1ebe-4264-a52c-7ad6f384fd75 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-df926b9c-1ebe-4264-a52c-7ad6f384fd75');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s--yUqAkhRBO"
   },
   "source": [
    "8\\. Напишите функцию, которая для заданного предложения выводит информацию о частях речи слов, входящих в предложение, в следующем виде:\n",
    "```\n",
    "PRP   VBD   DT      NNS     CC   VBD      NNS        RB   \n",
    " I  omitted the raspberries and added strawberries instead\n",
    "``` \n",
    "Для определения части речи слова можно воспользоваться `nltk.pos_tag`.\n",
    "\n",
    "Проверьте работоспособность функции на названии рецепта с id 241106.\n",
    "\n",
    "Обратите внимание, что часть речи должна находиться ровно посередине над соотвествующим словом, а между самими словами должен быть ровно один пробел.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize"
   ],
   "metadata": {
    "id": "JqQ1A6MD0DRa"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tag(sentence: str) -> str:\n",
    "  tokenizer = word_tokenize(sentence)\n",
    "  tagged_list = pos_tag(tokenizer)\n",
    "\n",
    "  res_word = ''\n",
    "  res_tag = ''\n",
    "\n",
    "  for elem in tagged_list:\n",
    "    word, tag = elem[0], elem[1]\n",
    "    n_spaces = abs(len(word) - len(tag))\n",
    "    if len(word) >= len(tag):\n",
    "      tag = ' ' * (n_spaces//2) + tag + ' ' * (n_spaces - n_spaces//2)\n",
    "    else:\n",
    "      word = ' ' * (n_spaces//2) + tag + ' ' * (n_spaces - n_spaces//2)\n",
    "      \n",
    "    res_tag += tag + ' '\n",
    "    res_word += word + ' '\n",
    "\n",
    "  return '\\n'.join([res_tag, res_word])\n"
   ],
   "metadata": {
    "id": "raju2qZ8yPd-"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "f5uffEJxhRBO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c65e2ec1-1dd2-4ac2-ea5a-49a9171a8781"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   JJ     NNS    IN     NNS    VBP    JJ   CC   JJ    NNS   \n",
      "eggplant steaks with chickpeas feta cheese and black olives \n"
     ]
    }
   ],
   "source": [
    "sentence = df_recipes[df_recipes['id'] == 241106]['name'].tolist()[0]\n",
    "print(tag(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}